# Domino EKS Cluster Upgrades
## Cluster
The EKS Cluster version is controlled by the eks.version value in the Domino CDK config:

    [...]
    eks:
      # version: "1.21" - Kubernetes version for EKS cluster. _MUST BE A STRING_!
      version: '1.21'
    [...]

### Using Domino CDK
To upgrade deployments provisioned directly through the Domino CDK app, you can edit this value directly and then run `cdk deploy`. If you started at '1.20', you can change it to '1.21' to upgrade to `1.21`, etc.

### Using the Domino Deployer
The deployer creates a cdk config for you located in `deploy-directory/cdk/config.yaml`, but this can also be regenerated from your deployment's `deploy.yaml` via the `provision generate --force` command.

Each release of the Domino CDK app, which the deployer utilizes to do CDK deployments, will have its own default version specified for its EKS cluster. Thus, you may want to manually set this version, or you may simply want to switch to the default.

#### Manually Selecting a Version
This process is essentially the same as when using CDK directly: Edit `deploy-directory/cdk/config.yaml`, set the `eks.version` variable to your desired EKS version, and then run `./bin/domino-deployment terraform provision` to update the running cluster.

#### Regenerating the Configuration
You can regenerate your CDK config with the `./bin/domino-deployment terraform generate --force` command. This will completely **reset** everything in the `cdk/`
subdirectory of your deployment directory and regenerate them based on your `deploy.yaml`. If you want to pick up the newest suggested default version, this will update it for you without you having to select it explicitly.

If you intend manage your EKS Cluster version explicitly going forward, you may also consider setting the CDK config override in your `deploy.yaml`:

    cdk:
      overrides:
        eks:
          version: '1.21'

This setting will be merged into the CDK config that is generated by the `terraform generate` command, and thus you can persist a specific EKS Cluster version through subsequent runs of `generate`.

As with manually editing the CDK config, you can affect these changes by running `./bin/domino-deployment terraform provision`.

### Cluster Add-ons
There are three services installed into every EKS cluster, `vpc-cni`, `kube-proxy` and `coredns`. We then re-install them as "Cluster Add-ons", which causes EKS to manage them on cluster upgrades. Consequently, they do not need to be manually updated.

## Nodegroups
Updating Managed and Unmanaged nodegroups are distinctly different processes.

Note that there will be downtime regardless of which type of Nodegroup is being upgraded. Rolling updates are minimally disruptive, but there will be unavoidable instances of unique services being stoped and restarted as they migrate between nodes.

Upgrading a `platform` nodegroup will cause downtime for the app itself, while updating a `compute` or `gpu` nodegroup will cause downtime for (or in some cases stop or cancel) all individual workspaces, runs, builds and models.

Some services may be distributed (ie you may have a model with a large number of replicas), but as EKS isn't app-aware, no effort or guarantee is made for uptime. Likewise, there are platform components that aren't replicated (ie git server).

### Managed Nodegroups
Managed nodegroups will not be updated automatically when updating the EKS cluster.

Once the cluster is upgraded, use the AWS Console to view your EKS cluster, and select the `Configuration` section. Within the `Configuration` section, view the `Compute` tab and you will see a list of all managed nodegroups.

To upgrade any given nodegroup, click the `Update now` link beside the `AMI release version` or view the nodegroup and click `Update now` on the notification at the top.

By default, EKS will use a "Rolling update" strategy, and cordon/drain nodes to ensure minimal downtime when performing upgrades, though a force option is also available.

### Unmanaged Nodegroups

After upgrading the EKS cluster in the previous section, the launch template should be updated to default to the newest EKS version. You have multiple strategies you can employ.

#### Instance Refresh (Forced Update)
For each Autoscaling Group, there is an option `Instance refresh`. This will forcibly replace each node in the nodegroup, one at a time. There is no cordon/drain step, so as Kubernetes becomes aware of missing nodes it will reschedule them on the new nodes that show up in the ASG.

This is the easiest method of updating an Unmanaged Nodegroup, however, it is also the least nuanced. If downtime and service interruption doesn't absolutely need to be minimized, we recommend this method.

#### Manual Cordoning/Draining
You can edit the ASG to overprovision (ie increase the "min" instances by one more than is needed), and then use `kubectl` to selectively cordon/drain nodes, and then delete them, one by one. This will provide the least downtime when upgrading an Unmanaged Nodegroup, but it is a very tedious, manual process, and does not by any means avoid downtime completely.

#### Replacement ASGs
Another variation of "Manual Cordoning/Draining" is to provision a new, "replacement" ASG (ie `platform-1.21`, `compute-1.21`, etc.), set the max nodes to the "current" amount of nodes in the old ASG, and then cordon and drain all the nodes from the original ASG all at once. After the old ASG has been emptied of running pods, remove the original ASG from the configuration and reprovision to remove it.

This requires less manual work than "Manual Cordoning/Draining", however, it does mean paying for extra resources during the transition.
